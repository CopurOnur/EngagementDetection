{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Engagement_Detection_OpenFace_Bi-LSTM_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNDgKHNK8mHLqlEI4QyR7kT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6091060c58d042b4b9280237e8cf1271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2245f714dbe34d7ca91e038699cc40b2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f1c511ae88654d34b574f76de1d36815",
              "IPY_MODEL_ee33446db20e49b98f8c8b071f656cd2",
              "IPY_MODEL_8b3e22d2594e485c88ebbe87c720cd68"
            ]
          }
        },
        "2245f714dbe34d7ca91e038699cc40b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1c511ae88654d34b574f76de1d36815": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_65ea99d2177a4337b4736d81aac98149",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab415556f5da45eb8d9b1306505eae33"
          }
        },
        "ee33446db20e49b98f8c8b071f656cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ff1bb26ca5c54c02997783ce8ac0aa4c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 15,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 15,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b2efa44dcf8247abb5776e9c96b1512f"
          }
        },
        "8b3e22d2594e485c88ebbe87c720cd68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c976ec42f1974c6f92c82c556f364ea7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 15/15 [00:00&lt;00:00, 23.35it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fb68d445938d4b559a0f25541cb7efc2"
          }
        },
        "65ea99d2177a4337b4736d81aac98149": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab415556f5da45eb8d9b1306505eae33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ff1bb26ca5c54c02997783ce8ac0aa4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b2efa44dcf8247abb5776e9c96b1512f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c976ec42f1974c6f92c82c556f364ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fb68d445938d4b559a0f25541cb7efc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CopurOnur/Engagement_Detection_OpenFace_Bi-LSTM/blob/main/Engagement_Detection_OpenFace_Bi_LSTM_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ME5DLWXEJOB"
      },
      "source": [
        "# Engagement Detection in E-Learning Environmets\n",
        "\n",
        "This notebook presents the code for my thesis named \"Engagement Detection in E-Learning Environments\". Before running the notebook, please change your run time type to \"GPU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFiYgn2HEqUl"
      },
      "source": [
        "## Load libraries (Notebook restart required after running the cell bellow)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrG1NhklD6Ws",
        "outputId": "6ca01d7d-83e6-4cc1-fe60-f876cccc8be1"
      },
      "source": [
        "!nvidia-smi\n",
        "!pip install --quiet torch\n",
        "!pip install --quiet pytorch-lightning\n",
        "!pip install scipy>=1.5\n",
        "! pip install stumpy\n",
        "! pip install --quiet tsfresh\n",
        "!pip install --quiet captum\n",
        "!pip install -U kora"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec 14 15:25:26 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\u001b[K     |████████████████████████████████| 525 kB 15.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 63.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 332 kB 60.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 50.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 51.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 69.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 192 kB 69.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 160 kB 62.2 MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting stumpy\n",
            "  Downloading stumpy-1.10.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 14.8 MB/s \n",
            "\u001b[?25hCollecting scipy>=1.5\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numba>=0.48 in /usr/local/lib/python3.7/dist-packages (from stumpy) (0.51.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from stumpy) (1.19.5)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.48->stumpy) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.48->stumpy) (57.4.0)\n",
            "Installing collected packages: scipy, stumpy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed scipy-1.7.3 stumpy-1.10.0\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 802 kB 32.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 49.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 802 kB 62.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 793 kB 61.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 793 kB 48.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 791 kB 53.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 786 kB 62.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 779 kB 56.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 778 kB 59.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 776 kB 51.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 769 kB 35.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 766 kB 55.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 54.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 722 kB 39.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 722 kB 62.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 715 kB 60.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 705 kB 38.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 699 kB 61.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 696 kB 41.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 684 kB 65.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 679 kB 68.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 675 kB 67.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 675 kB 38.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 672 kB 65.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 671 kB 63.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 669 kB 67.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 656 kB 52.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 71.3 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.4.0 requires protobuf<4,>=3.13, but you have protobuf 3.11.2 which is incompatible.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\n",
            "googleapis-common-protos 1.53.0 requires protobuf>=3.12.0, but you have protobuf 3.11.2 which is incompatible.\n",
            "google-api-core 1.26.3 requires protobuf>=3.12.0, but you have protobuf 3.11.2 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 14.2 MB/s \n",
            "\u001b[?25hCollecting kora\n",
            "  Downloading kora-0.9.19-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting fastcore\n",
            "  Downloading fastcore-1.3.27-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from kora) (5.5.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastcore->kora) (21.1.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastcore->kora) (21.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (57.4.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->kora) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->kora) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->fastcore->kora) (3.0.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->kora) (0.7.0)\n",
            "Installing collected packages: fastcore, kora\n",
            "Successfully installed fastcore-1.3.27 kora-0.9.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DSsCC8gAyDS"
      },
      "source": [
        "### Run this cell for restart"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR8HVi4o-qpi"
      },
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZWHcF8t9qH4"
      },
      "source": [
        "The code chunk bellow access my shared google dirve folder and the necessary files. For authentications you need to sign in with your google account but dont worry, it is not mounting to your drive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_1_5Ng-6psk"
      },
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()  # must authenticate\n",
        "\n",
        "\n",
        "'''list all ids of files directly under folder folder_id'''\n",
        "\n",
        "def folder_list(folder_id):\n",
        "\n",
        "  from googleapiclient.discovery import build\n",
        "\n",
        "  gdrive = build('drive', 'v3').files()\n",
        "\n",
        "  res = gdrive.list(q=\"'%s' in parents\" % folder_id).execute()\n",
        "\n",
        "  return [f['id'] for f in res['files']]\n",
        "\n",
        "\n",
        "\n",
        "'''download all files from a gdrive folder to current directory'''\n",
        "\n",
        "def folder_download(folder_id):\n",
        "\n",
        "  for fid in folder_list(folder_id):\n",
        "\n",
        "    !gdown -q --id $fid\n",
        "\n",
        "link='https://drive.google.com/drive/folders/1nfh-Qj2xUE5F5qRYiLIVq2RvK8UdU5LX?usp=sharing'\n",
        "\n",
        "\n",
        "folder_id=\"1nfh-Qj2xUE5F5qRYiLIVq2RvK8UdU5LX\"\n",
        "\n",
        "folder_download(folder_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM28Q4TzFAu7"
      },
      "source": [
        "## OpenFace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sQR1m0WFEKE"
      },
      "source": [
        "First, the video features are extracted through openface. The code chunck bellow downloads OpenFace and converts the input video into csv file. Downloading OpenFace takes quite some time so you can use \"content/mrslowack.csv\" to run the model. However, if you want to use a different video, you need to download openface and run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTl6alTyFCrm"
      },
      "source": [
        "import os\n",
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "################# Need to revert back to CUDA 10.0 ##################\n",
        "# Thanks to http://aconcaguasci.blogspot.com/2019/12/setting-up-cuda-100-for-mxnet-on-google.html\n",
        "#Uninstall the current CUDA version\n",
        "!apt-get --purge remove cuda nvidia* libnvidia-*\n",
        "!dpkg -l | grep cuda- | awk '{print $2}' | xargs -n1 dpkg --purge\n",
        "!apt-get remove cuda-*\n",
        "!apt autoremove\n",
        "!apt-get update\n",
        "\n",
        "#Download CUDA 10.0\n",
        "!wget  --no-clobber https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\n",
        "#install CUDA kit dpkg\n",
        "!dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\n",
        "!sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda-10-0\n",
        "#Slove libcurand.so.10 error\n",
        "!wget --no-clobber http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
        "#-nc, --no-clobber: skip downloads that would download to existing files.\n",
        "!apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
        "!apt-get update\n",
        "####################################################################\n",
        "\n",
        "git_repo_url = 'https://github.com/TadasBaltrusaitis/OpenFace.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "# clone openface\n",
        "!git clone -q --depth 1 $git_repo_url\n",
        "\n",
        "# install new CMake becaue of CUDA10\n",
        "!wget -q https://cmake.org/files/v3.13/cmake-3.13.0-Linux-x86_64.tar.gz\n",
        "!tar xfz cmake-3.13.0-Linux-x86_64.tar.gz --strip-components=1 -C /usr/local\n",
        "\n",
        "# Get newest GCC\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install build-essential \n",
        "!sudo apt-get install g++-8\n",
        "\n",
        "# install python dependencies\n",
        "!pip install -q youtube-dl\n",
        "\n",
        "# Finally, actually install OpenFace\n",
        "!cd OpenFace && bash ./download_models.sh && sudo bash ./install.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr_d2KZo1VDf"
      },
      "source": [
        " you can set the path of your new video bellow and the output will be saved to '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHbnsEjmE_OB"
      },
      "source": [
        "video = '/content/pretended.mp4'\n",
        "newpath = '/content/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMzol75gsUE5"
      },
      "source": [
        "! ./OpenFace/build/bin/FeatureExtraction -f $video -out_dir $newpath"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuE1lorRGNRG"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_1ZQX2OGL_n",
        "outputId": "dbbba9d6-925e-4d7a-846d-1c2c0c60cdd5"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(1, '/content')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from multiprocessing import cpu_count\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from scipy import fftpack\n",
        "\n",
        "import math\n",
        "import utils\n",
        "import itertools\n",
        "import torchmetrics\n",
        "accuray = torchmetrics.Accuracy()\n",
        "\n",
        "import sys\n",
        "import dataloader\n",
        "import model_train\n",
        "import random\n",
        "from tsfresh.feature_extraction import extract_features, MinimalFCParameters,feature_calculators\n",
        "from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\n",
        "from captum.attr import IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  warnings.warn(problem)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKS8GTY9HBnA"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#93D30C\", \"#8F00FF\"]\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "rcParams['figure.figsize'] = 16, 10"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_DYLr-T1-a9"
      },
      "source": [
        "# Initial Variables\n",
        "Please define the variables bellow before running the code. the \"csv\" variable refers to the extracted openface features. \"raw_video\" should be the name of the input video. \"pic_folder\" is the name of the folder that the frames with engagement scores will be saved. After that the these frames will be converted to the output video. \"out_video\" is the name of the output video. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jO6SDv5W9jM"
      },
      "source": [
        "folder = '/content/'\n",
        "csv = 'pretend.csv'\n",
        "raw_video = 'pretend.mp4'\n",
        "pic_folder = \"labeled\"\n",
        "out_video = \"out.mp4\"\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quoK_BY02389"
      },
      "source": [
        "# DataLoader\n",
        "the class bellow is the data dataloader for the model. it takes the input csv, divides it into number of sub clips in which an engagement level will be assigned to each clip. We decided to give assign an engagement level for eac 130 frames reprsented by \"self.frame_per_clip\" variable. The \"self.frame_size\" variable represents the number of sequences after statistical aggregation for each clip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsGYFtVoIR_N"
      },
      "source": [
        "class OpenFaceDataset(Dataset):\n",
        "    ''' Load dataset as torch.tensor '''\n",
        "    def __init__(self, root=folder + csv):\n",
        "        self.frame_per_clip = 110\n",
        "        self.csv = pd.read_csv(root)\n",
        "        self.frame_size =  5\n",
        "        self.overlap_size = int(self.frame_per_clip/(self.frame_size*5))\n",
        "\n",
        "\n",
        "        self.gaze_range=[4,10]\n",
        "        self.head_range = [10,13]\n",
        "        self.rot_range = [13,16]\n",
        "        self.aus_range = [-35,-18]\n",
        "        self.attributes = [\"gaze_seg\",\n",
        "        \"head_seg\",\n",
        "        \"rot_seg\",\n",
        "        \"aus_seg\"\n",
        "        ]\n",
        "        self.functions = [\"length\",\n",
        "                          \"maximum\",\"minimum\",\"variance\"\n",
        "        #,\"mean_change\"\n",
        "        ]\n",
        "\n",
        "        self.file_list = self.split_video()\n",
        "        self.all_features = self.get_feature()\n",
        "        \n",
        "\n",
        "\n",
        "    def split_video(self):\n",
        "      clips = []\n",
        "      limit = self.csv.shape[0]\n",
        "      step = int(self.csv.shape[0]/self.frame_per_clip)\n",
        "      clip_idx = np.linspace(0,limit,step+1,dtype=int)\n",
        "      for i in range(len(clip_idx)-1):\n",
        "        if i==0:\n",
        "          seg = self.csv.iloc[clip_idx[i]:clip_idx[i+1],:]\n",
        "        else:\n",
        "          seg = self.csv.iloc[clip_idx[i]- self.overlap_size :clip_idx[i+1] - self.overlap_size,:]\n",
        "        clips.append(seg)\n",
        "      return clips\n",
        "\n",
        "\n",
        "    def get_feature(self):\n",
        "        features = []\n",
        "        for idx in range(len(self.file_list)):\n",
        "            # segment video to 10 segments, return features\n",
        "            file_dir, label = self.file_list[idx], None\n",
        "            v_data = np.array(file_dir)\n",
        "            v_data = np.delete(v_data, 0, 0)    # delete table caption\n",
        "            v_data = v_data.astype(np.float)   # gaze / pose\n",
        "\n",
        "            # remove nan\n",
        "            v_data = v_data[~np.isnan(v_data).any(axis=1)]\n",
        "            #scaler = MinMaxScaler(feature_range=(0,1))\n",
        "            #v_data = scaler.fit_transform(v_data)\n",
        "            #print(v_data.shape)\n",
        "\n",
        "            limit = v_data.shape[0]\n",
        "            step = self.frame_size\n",
        "            frame_idx = np.linspace(0,limit,step+1,dtype=int)\n",
        "            #print(\"frames \",frame_idx)\n",
        "            feature = []\n",
        "            for i in range(len(frame_idx)-1):\n",
        "              seg = v_data[frame_idx[i]:frame_idx[i+1],:]\n",
        "              gaze_seg = seg[:,self.gaze_range[0]:self.gaze_range[1]]\n",
        "              head_seg = seg[:,self.head_range[0]:self.head_range[1]]\n",
        "              rot_seg = seg[:,self.rot_range[0]:self.rot_range[1]]\n",
        "              aus_seg = seg[:,self.aus_range[0]:self.aus_range[1]]\n",
        "\n",
        "              selected_feature=[]\n",
        "              for att in self.attributes:\n",
        "                for func in self.functions:\n",
        "                  method_to_call = getattr(feature_calculators, func)\n",
        "                  selected_feature.append(np.apply_along_axis(method_to_call,0,locals()[att]))\n",
        "\n",
        "              feature.append(torch.FloatTensor(np.concatenate(selected_feature)))\n",
        "            features.append(feature)\n",
        "\n",
        "        return features\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.all_features[idx]\n",
        "\n",
        "        data = torch.zeros((self.frame_size,len(x[0])))\n",
        "        for i in range(self.frame_size):\n",
        "            data[i,:] = x[i]\n",
        "        \n",
        "        return dict(\n",
        "          sequence = data, #torch.reshape(data,(44,self.frame_size)),\n",
        "          label = 1)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def get_labels(self):\n",
        "  \n",
        "      return self.label_list\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOpvj7kuXP3h"
      },
      "source": [
        "\n",
        "class OpenFaceDataModule(pl.LightningDataModule):\n",
        "  def __init__(self, batch_size):\n",
        "    super().__init__()\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def setup(self, stage=None):\n",
        "    self.train_dataset = OpenFaceDataset()\n",
        "    self.test_dataset = OpenFaceDataset()\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.train_dataset,\n",
        "        batch_size=self.batch_size,\n",
        "        #sampler=ImbalancedDatasetSampler(self.train_dataset),\n",
        "        shuffle=True,\n",
        "        num_workers=cpu_count()\n",
        "    )\n",
        "  \n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.test_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=1\n",
        "    )\n",
        "  \n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.test_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=1\n",
        "    )\n",
        "  \n",
        "    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5PFbtd2XR4V"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "data_module = OpenFaceDataModule(BATCH_SIZE)\n",
        "data_module.setup()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6joF_uf4C7m"
      },
      "source": [
        "# Model\n",
        "Bellow, you can see the code chunk for the Bi-LSTM model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_EAgDu3RugT"
      },
      "source": [
        "class SequenceModel(nn.Module):\n",
        "  def __init__(self, n_features, n_hidden=512, n_layers=2,dropout=0.3, freeze_lstm = False):\n",
        "    super().__init__()\n",
        "    self.n_hidden = n_hidden\n",
        "    self.n_layers = n_layers\n",
        "    self.dropout = dropout\n",
        "    self.freeze_lstm = freeze_lstm\n",
        "\n",
        "    def weight_init(m):\n",
        "      if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "    self.cnn1d = nn.Sequential(\n",
        "        nn.Conv1d(44,16,3,padding_mode=\"replicate\"),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(16,8,3,padding_mode=\"replicate\"),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    \n",
        "    self.mlp = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(self.n_hidden*2 , 128),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32,1)\n",
        "    )\n",
        "    #self.mlp.apply(weight_init)\n",
        "    self.lstm = nn.LSTM(\n",
        "         input_size= n_features,\n",
        "         hidden_size=self.n_hidden,\n",
        "         num_layers=self.n_layers,\n",
        "         batch_first=True,\n",
        "         dropout = self.dropout,\n",
        "         bidirectional = True\n",
        "         )\n",
        "    if freeze_lstm:\n",
        "      for param in self.lstm.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    #xr = torch.reshape(self.cnn1d(x),(-1,12,8))\n",
        "    xr=x\n",
        "    h0 = torch.zeros(self.n_layers*2, xr.size(0), self.n_hidden)\n",
        "    h0= h0.type_as(x)\n",
        "    c0 = torch.zeros(self.n_layers*2, xr.size(0), self.n_hidden)\n",
        "    c0= c0.type_as(x)\n",
        "    out,_ = self.lstm(xr,(h0,c0))\n",
        "    out= out.type_as(x)\n",
        "    out = self.mlp(out[:,-1, :])\n",
        "    return out\n",
        "\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaqo3NXvXY8U"
      },
      "source": [
        "class EngagementPredictor(pl.LightningModule):\n",
        "\n",
        "  def __init__(self, n_features: int):\n",
        "    super().__init__()\n",
        "    self.model=SequenceModel(n_features)\n",
        "    self.criterion = nn.MSELoss()\n",
        "\n",
        "  def forward(self, x, labels=None):\n",
        "    output=self.model(x)\n",
        "    loss=0\n",
        "    if labels is not None:\n",
        "      loss=self.criterion(output,labels.unsqueeze(dim=1))\n",
        "      return loss, output\n",
        "    else:\n",
        "      return output\n",
        "      \n",
        "  \n",
        "  def training_step(self, batch, batch_idx):\n",
        "    sequences = batch[\"sequence\"]\n",
        "    labels = batch[\"label\"]\n",
        "    loss, outputs = self.forward(sequences, labels)\n",
        "    self.log(\"train_loss\",loss, prog_bar=True, logger=True)\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    sequences = batch[\"sequence\"]\n",
        "    labels = batch[\"label\"]\n",
        "    loss, outputs = self.forward(sequences, labels)\n",
        "    self.log(\"validation_loss\",loss, prog_bar=True, logger=True)\n",
        "    return loss\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    sequences = batch[\"sequence\"]\n",
        "    labels = batch[\"label\"]\n",
        "    loss, outputs = self.forward(sequences, labels)\n",
        "    self.log(\"test_loss\",loss, prog_bar=True, logger=True)\n",
        "    return loss\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return optim.AdamW(self.parameters(), lr=0.0001)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxBdcgITXbiM"
      },
      "source": [
        "model = EngagementPredictor(\n",
        "    n_features = data_module.train_dataset[0][\"sequence\"].shape[1])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyd1Rz414O1L"
      },
      "source": [
        "# Load the pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0S1uAUSXd2U"
      },
      "source": [
        "trained_model = EngagementPredictor.load_from_checkpoint(\n",
        "    \"/content/trained_model_weights.ckpt\",\n",
        "    n_features = data_module.train_dataset[0][\"sequence\"].shape[1],\n",
        "    n_hidden = 512,\n",
        "    n_layers = 2,\n",
        "    dropout = 0.3)\n",
        "\n",
        "trained_model.freeze()\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6091060c58d042b4b9280237e8cf1271",
            "2245f714dbe34d7ca91e038699cc40b2",
            "f1c511ae88654d34b574f76de1d36815",
            "ee33446db20e49b98f8c8b071f656cd2",
            "8b3e22d2594e485c88ebbe87c720cd68",
            "65ea99d2177a4337b4736d81aac98149",
            "ab415556f5da45eb8d9b1306505eae33",
            "ff1bb26ca5c54c02997783ce8ac0aa4c",
            "b2efa44dcf8247abb5776e9c96b1512f",
            "c976ec42f1974c6f92c82c556f364ea7",
            "fb68d445938d4b559a0f25541cb7efc2"
          ]
        },
        "id": "zwtvuCmlXfeA",
        "outputId": "7d2c64fb-27f1-44dc-c73f-84873443f036"
      },
      "source": [
        "labels = []\n",
        "predictions = []\n",
        "\n",
        "for item in tqdm(data_module.val_dataloader()):\n",
        "  sequence = item[\"sequence\"]\n",
        "  label = item[\"label\"]\n",
        "  _, output = trained_model(sequence,label)\n",
        "  predictions.append(output)\n",
        "  labels.append(label.item())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6091060c58d042b4b9280237e8cf1271",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUrBgXgs4hHj"
      },
      "source": [
        "# Put Engagement Labels on the video frames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZmeVs6EMUF0"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "cap = cv2.VideoCapture(folder + raw_video)\n",
        "count = 0\n",
        "limit = data_module.train_dataset.csv.shape[0]\n",
        "step = len(predictions)\n",
        "tresh = np.linspace(0,limit,step+1,dtype=int)\n",
        "while(True):\n",
        "      \n",
        "    # Capture frames in the video\n",
        "    ret, frame = cap.read()\n",
        "  \n",
        "    # describe the type of font\n",
        "    # to be used.\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "  \n",
        "    # Use putText() method for\n",
        "    # inserting text on video\n",
        "    for i in range(1,step+1):\n",
        "      if count<tresh[i]:\n",
        "        acc = predictions[i-1]\n",
        "        count+=1\n",
        "        break\n",
        "\n",
        "    cv2.putText(frame, \n",
        "                'Engagement Level is '+ str(acc.item()), \n",
        "                (25, 25), \n",
        "                font, 1, \n",
        "                (0, 255, 255), \n",
        "                2, \n",
        "                cv2.LINE_4)\n",
        "\n",
        "    from pathlib import Path\n",
        "    Path(folder + pic_folder).mkdir(parents=True, exist_ok=True)\n",
        "    # Display the resulting frame\n",
        "    try:\n",
        "      cv2.imwrite(folder + pic_folder+ \"/frame%d.jpg\" % count, frame)\n",
        "    except:\n",
        "      break\n",
        "  \n",
        "# release the cap object\n",
        "cap.release()\n",
        "# close all windows\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9T9KU_E4z_E"
      },
      "source": [
        "# Convert the labeled frames to video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3OQGuKTYO6E"
      },
      "source": [
        "import glob\n",
        "import natsort\n",
        "sorted = natsort.natsorted(os.listdir(folder + pic_folder),reverse=False)\n",
        "img_array = []\n",
        "for filename in sorted:\n",
        "    img = cv2.imread(folder + pic_folder + \"/\" + filename)\n",
        "    height, width, layers = img.shape\n",
        "    size = (width,height)\n",
        "    img_array.append(img)\n",
        " \n",
        " \n",
        "out = cv2.VideoWriter('project.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
        " \n",
        "for i in range(len(img_array)):\n",
        "    out.write(img_array[i])\n",
        "out.release()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F5lUv47Yygu",
        "outputId": "278079e8-c20c-443b-ad16-7c5243374c2b"
      },
      "source": [
        "!ffmpeg -i project.avi output.mp4"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, avi, from 'project.avi':\n",
            "  Metadata:\n",
            "    encoder         : Lavf58.35.100\n",
            "  Duration: 00:01:56.53, start: 0.000000, bitrate: 2033 kb/s\n",
            "    Stream #0:0: Video: mpeg4 (Simple Profile) (DIVX / 0x58564944), yuv420p, 1280x720 [SAR 1:1 DAR 16:9], 2031 kb/s, 15 fps, 15 tbr, 15 tbn, 15 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=15 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 15 fps, 15360 tbn, 15 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame= 1748 fps= 15 q=-1.0 Lsize=   20355kB time=00:01:56.33 bitrate=1433.4kbits/s speed=0.97x    \n",
            "video:20341kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.069869%\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0mframe I:8     Avg QP:18.30  size: 47208\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0mframe P:1299  Avg QP:20.47  size: 13908\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0mframe B:441   Avg QP:22.33  size:  5405\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0mconsecutive B-frames: 58.5% 15.9% 22.8%  2.7%\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0mmb I  I16..4: 10.9% 84.0%  5.0%\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0mmb P  I16..4:  1.7% 10.7%  0.1%  P16..4: 55.5%  9.1%  5.1%  0.0%  0.0%    skip:17.9%\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0mmb B  I16..4:  0.8%  6.9%  0.0%  B16..8: 34.6%  1.7%  0.2%  direct: 2.6%  skip:53.1%  L0:57.9% L1:36.4% BI: 5.7%\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0m8x8 transform intra:86.5% inter:86.4%\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0mcoded y,uvDC,uvAC intra: 60.5% 63.0% 5.5% inter: 21.8% 31.0% 0.4%\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0mi16 v,h,dc,p: 27% 15% 38% 20%\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 26% 18% 47%  2%  1%  2%  1%  1%  2%\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 41% 22% 13%  3%  5%  6%  4%  3%  2%\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0mi8c dc,h,v,p: 44% 23% 29%  4%\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0mWeighted P-Frames: Y:0.7% UV:0.6%\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0mref P L0: 75.0%  9.7% 11.7%  3.6%  0.0%\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0mref B L0: 86.1% 12.0%  1.9%\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0mref B L1: 99.8%  0.2%\n",
            "\u001b[1;36m[libx264 @ 0x5610798bfe00] \u001b[0mkb/s:1429.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAd9E0_Q48WP"
      },
      "source": [
        "# Display the Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "ZZe8xtvlZJef",
        "outputId": "55632263-d121-43ab-a97f-6c0baef8e63c"
      },
      "source": [
        "from kora.drive import upload_public\n",
        "url = upload_public('/content/output.mp4')\n",
        "# then display it\n",
        "from IPython.display import HTML\n",
        "HTML(f\"\"\"<video src={url} width=500 controls/>\"\"\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<video src=https://drive.google.com/uc?id=1BPeMSmxFul_XAy_j5J7CvI2CXiHe5zjq width=500 controls/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}